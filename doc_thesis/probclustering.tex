\documentclass[11pt]{article}
\usepackage[margin=1in, paperwidth=8.5in, paperheight=11in]{geometry}
\usepackage{epstopdf}
\usepackage[parfill]{parskip}
\usepackage[pdftex]{graphicx}
\usepackage{changepage}
\usepackage{titling}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{bm}
\usepackage{extramarks}
\usepackage{amsfonts}
\usepackage{amsthm}
\usepackage{array}
\usepackage{framed}
\usepackage{xy}
\usepackage{multicol}
\usepackage{hyperref}
\usepackage{float}
\usepackage{subfig}
\usepackage[algoruled]{algorithm2e}
\textwidth 7.0in
\usepackage{fancyhdr}
\usepackage{lastpage}
\pagestyle{fancy}
\headheight 26pt
\fancyhf{}
\linespread{1.1}
\headsep20pt
\newcommand{\inlinecode}{\texttt}
\newcommand{\divider}{\line(1,0){250}}
\rfoot{Page \thepage \  of \pageref{LastPage} }

\renewcommand\headrulewidth{0.4pt}
\renewcommand\footrulewidth{0.4pt}

\setlength\parindent{0pt}

%Custom definitions
\newcommand{\problempart}[1]{\textbf{#1.}}
\newcommand{\pof}[1]{\text{P(#1)}}
\newcommand{\subtitle}[1]{%
      \posttitle{%
              \par\end{center}
                  \begin{center}\large#1\end{center}
                          \vskip0.5em}%
                      }

%%Set symbols
\newcommand{\given}{\ensuremath{|} }
\newcommand{\AND}{\ensuremath{\cap} }
\newcommand{\OR}{\ensuremath{\cup} }

\newcommand{\N}{\mathbb{N}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\Q}{\mathbb{Q}}

\newcommand{\powerset}[1]{\ensuremath{ \mathcal P \left({#1}\right)}}
\newcommand{\set}[1]{\ensuremath{\left\{ #1 \right\}}}
\newcommand{\stcomp}[1]{\overline{#1}} 

\newcommand{\dlim}[2][\infty]{\displaystyle \lim_{#2 \rightarrow #1}}
\newcommand{\indefint}{\displaystyle \int}
\newcommand{\be}{\begin{enumerate}}
\newcommand{\ee}{\end{enumerate}}
\newcommand{\dsum}[2]{\displaystyle \sum_{#1}^{#2} }
\newcommand{\defint}[4]{\int^#2_#1 #3\,d#4}

\newcommand{\bx}{\ensuremath{\mathbf{x}}}
\newcommand{\bz}{\ensuremath{\mathbf{z}}}
\newcommand{\kl}[1]{\textsc{kl}\left(#1\right)}
\newcommand{\g}{\,\vert\,}

\newcommand{\EE}[1]{\mathbb{E}\left[#1\right]}
\newcommand{\EEE}[2]{\mathbb{E}_{#1}\left[#2\right]}
\newcommand{\ELBO}{\textsc{elbo}}

%
% Create Problem Sections
%

\newcommand{\enterProblemHeader}[1]{
    \nobreak\extramarks{}{Problem \arabic{#1} continued on next page\ldots}\nobreak{}
    \nobreak\extramarks{Problem \arabic{#1} (continued)}{Problem \arabic{#1} continued on next page\ldots}\nobreak{}
}

\newcommand{\exitProblemHeader}[1]{
    \nobreak\extramarks{Problem \arabic{#1} (continued)}{Problem \arabic{#1} continued on next page\ldots}\nobreak{}
    \stepcounter{#1}
    \nobreak\extramarks{Problem \arabic{#1}}{}\nobreak{}
}

\setcounter{secnumdepth}{0}
\newcounter{partCounter}
\newcounter{homeworkProblemCounter}
\setcounter{homeworkProblemCounter}{1}
\nobreak\extramarks{Problem \arabic{homeworkProblemCounter}}{}\nobreak{}

%
% Homework Problem Environment
%
% This environment takes an optional argument. When given, it will adjust the
% problem counter. This is useful for when the problems given for your
% assignment aren't sequential. See the last 3 problems of this template for an
% example.
%
\newenvironment{homeworkProblem}[1][-1]{
    \ifnum#1>0
        \setcounter{homeworkProblemCounter}{#1}
    \fi
    \section{Problem \arabic{homeworkProblemCounter}}
    \setcounter{partCounter}{1}
    \enterProblemHeader{homeworkProblemCounter}
}{
    \exitProblemHeader{homeworkProblemCounter}
}

\renewcommand{\part}[1]{\textbf{\large Part \Alph{partCounter}}\stepcounter{partCounter}\vspace{4pt}  \\}

% Various Helper Commands
%

\newcommand{\adjustimg}{% Horizontal adjustment of image
  \checkoddpage%
  \ifoddpage\hspace*{\dimexpr\evensidemargin-\oddsidemargin}\else\hspace*{-\dimexpr\evensidemargin-\oddsidemargin}\fi%
}
\newcommand{\centerimg}[2][width=\textwidth]{% Center an image
  \makebox[\textwidth]{\adjustimg\includegraphics[#1]{#2}}%
}


% Useful for algorithms
\newcommand{\alg}[1]{\textsc{\bfseries \footnotesize #1}}

% For derivatives
\newcommand{\deriv}[1]{\frac{\mathrm{d}}{\mathrm{d}x} (#1)}

% For partial derivatives
\newcommand{\pderiv}[2]{\frac{\partial}{\partial #1} (#2)}

% Integral dx
\newcommand{\dx}{\mathrm{d}x}

% Alias for the Solution section header
\newcommand{\solution}{\textbf{\large Solution}}

% Probability commands: Expectation, Variance, Covariance, Bias
\newcommand{\E}{\mathrm{E}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\Bias}{\mathrm{Bias}}

\newcommand{\matr}[1]{\mathbf{#1}}

\makeatletter
\def\thm@space@setup{%
  \thm@preskip=0.4cm
  \thm@postskip=\thm@preskip % or whatever, if you don't want them to be equal
}
\makeatother


\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{proposition}{Proposition}
\newtheorem{observation}{Observation}
\newtheorem{proposal}{Proposal}
\newtheorem{claim}{Claim}

% Move qed box to the left
\renewcommand{\qed}{\unskip\nobreak\quad\qedsymbol}

\newenvironment{conditions}
  {\par\vspace{\abovedisplayskip}\noindent\begin{tabular}{>{$}l<{$} @{${}={}$} l}}
  {\end{tabular}\par\vspace{\belowdisplayskip}}

% Title page

  \title{Clustering SNVs for Tumor Heterogeneity}
  \subtitle{Math-CS Sc.B Thesis Proposal}
  \date{\today}
\author{David Liu}
% Header

\begin{document}
\maketitle
\section{Problem statement}
Cancer results from an evolutionary process where somatic mutations occur and accumulate in a population of cells. A mutation causes genetic variation at a genomic site called a single nucleotide variant (SNV) for the cell which obtained the mutation and all of its progeny. Thus, combinations of SNVs correspond to different subpopulations of cells (clones) which can be placed on a phylogenetic tree. This mixture of clones is a phenomenon known as intratumor hetereogeneity. Each clone is commonly modeled by a binary feature vector, where each feature is an SNV.

So suppose we take biopsy samples from a tumor seperated spatially or temporally. Each sample will have different proportions of clones whose relationships are unknown. Our goal is to characterize the evolutionary history of the clones (invariant across samples) and their mixing proportions (variant across samples). 

The data typically used for tree inference is called the variant allele frequency (VAF), observed as follows. Suppose we bulk sequence each sample seperately, so that the reads are from a sample's mixture of clones. By comparing to a control sample, if a read contains the mutated allele at a SNV, it is called a variant read; otherwise it is called a reference read. The VAF is defined for each SNV in each sample, as, in each sample, the number of variant reads at an SNV divided by the number of total reads at that SNV.

There exist algorithms to infer a tree and its population mixing proportions given perfectly accurate VAFs. However in the real world, data is noisy and this isn't so simple. Two VAFs that look different may actually be from the same clone; likewise, two VAFs that look the same may be from different clones. Thus, it is common to perform clustering to assign mutations to clusters which correspond to clones. Tree inference is then performed on these clusters.

In this thesis, I propose and implement a method to cluster mutations using the Dirichlet Process and variational inference based on a binomial mixture model, which is suited for the clone mixing problem. It could also have applications in other clustering problems.

\newpage

\section{Model}

Let $m = 1, \ldots, M$ index the samples and let $n = 1, ..., N$ index the SNVs. For each SNV $n$ in sample $m$, we observe two quantities: the total number $d_{mn}$ of reads and the number $v_{mn}$ of variant reads. Let each data observation be a vector $\bx_n$, a vector which encapsulates $d_{mn}, v_mn$ for all $m$ and this $n$. Let $k=1,\ldots,K$ index the clusters and let $z_n$ denote the cluster assignment of SNV $n$, where $z_n$ is a 1-of-$K$ indicator vector. In addition, let $\bz$ be the vector of all $z_n$.

Further suppose that the cluster memberships $z_n$ and weights $\pi_k$ are generated by a Dirichlet process prior. Thus $K$ could be countably infinite, but with probability one, $K$ is finite. Thus, we restrict $K$ to a positive number, with all cluster indices greater than $K$ being irrelevant.

Suppose that each clone emits variant reads according to a binomial distribution. Thus, for cluster $k$ and some reads $d_{mn}$, we have variant reads distributed with $\mathrm{Bin}(d_{mn}, \phi_{kz_{nk}})$. (\textbf{Note: May change this into a negative binomial model.}) We can consider the joint probability of a site by the product of each sample, since we assume they are independent. That is,
\begin{equation}
p(\mathbf{x_n} | \mathbf{\phi_n}) = \prod\limits_{m=1}^M \mathrm{Bin}(d_{mn}, \phi_{mn})
\end{equation}

 As we can see by inspecting the graphical model, the likelihood of $\bx$ depends on the latent variables in a straightforward way:
\begin{align}
p(\mathbf{x_n} | \bz, \mathbf{\phi}) &= \prod\limits_{k=1}^K p(\mathbf{x_n} | \mathbf{\phi_n})^{z_{nk}} \nonumber \\
										&= \prod\limits_{k=1}^K \prod\limits_{m=1}^M \mathrm{Bin}(d_{mn}, \phi_{mn})^{z_{nk}}
\end{align}
Now consider the joint likelihood of the observed data and latent variables, which follows from (2): 
\begin{equation}
p(\mathbf{x_n}, \bz | \mathbf{\pi}, \mathbf{\phi_n}) = \prod\limits_{k=1}^K \prod\limits_{m=1}^M \left(\pi_k \mathrm{Bin}(d_{mn}, \phi_{mn})\right)^{z_{nk}}
\end{equation}

Figure~\ref{fig:GM} on the next page shows the graphical model.

\newpage
\begin{quote}
$n = 1, \ldots , N$:  SNVs \\
$m = 1, \ldots , M$: samples \\
$k = 1, \ldots ,  K$: clusters
\end{quote}

\begin{figure}[H]
\centerimg[scale=1.0]{multi_pgm.png}
\caption{Graphical model for the VAFs.}
\label{fig:GM}
\end{figure}

\begin{quote}
$DP$ = Dirichlet process RV (stick-breaking construction) \\
$H \sim U(0,1)$ = Base distribution for parameters $\phi_{nk}$	 \\
$\pi_k$ = Weights for categorical distribution \\
$z_{n} \in \{1, \cdots, K\}  \sim \mathrm{Categorical}(\pi_1, \cdots, \pi_K)$ =  Cluster membership for SNV $n$\\
$\phi_{mk}$ = Cluster frequency \\ 
$v_{mn} \sim \mathrm{Binom}(d_{m,n}, \phi_{m,c_n})$ = Observed variant reads\\ 
$d_{mn}$ = Observed total reads
\end{quote}
\vspace{0.05cm}

As described in (Blei 2006), the data arises in the following manner, with a stick-breaking construction given as:
\begin{align*}
\pi_i(\mathbf{w}) &= w_i \prod\limits_{j=1}^{i-1} (1 - v_j) \\
				DP &= \sum_{i=1}^\infty \pi_i(\mathbf{w}) \delta_{\phi_i}
\end{align*}
\begin{enumerate}
	\item Draw $W_k | \alpha \sim \mathrm{Beta}(1, \alpha)$, \hspace{1cm} $k = \{1, 2, \ldots\}$
	\item Draw $\phi_k | H \sim H^k$, \hspace{1cm} $k = \{1, 2, \ldots\}$
	\item For the $n$th data point:
	\begin{enumerate}
		\item Draw $z_n | \{\pi_1, \pi_2, \ldots\} \sim \mathrm{Mult}(\pi(\mathbf{w}))$.
		\item Draw $\mathbf{w_n} | z_n \sim p(x_n | \phi_n)$.
	\end{enumerate}
\end{enumerate}
\vspace{0.05cm}
The full posterior 
\begin{equation}
p(\bz | \bx, \alpha, H) = \int p(\bx | \mathbf{\phi}) p(\phi | \bx, \alpha, H) \, d\phi
\end{equation}
involves a Dirichlet Process and is thus analytically intractable. We must use some sort of computational technique to perform inference on this posterior.

\section{Variational Inference}

Variational inference is an alternative to MCMC-based inference methods. At a high level, variational inference factors a posterior using the mean-field approximation, which approximates the posterior in a higher-dimensional space using simpler independent functions. Then a simple coordinate ascent can be performed in order to infer the model parameters.

\section{Details on Variational Inference}

\subsection{The \textsc{ELBO}}

Let $\mathbf{z}$ denote the latent variables, and $\mathbf{x}$ denote the data. We seek to approximate the posterior $p(\mathbf{z}|\mathbf{x})$ from a family of distributions $\mathcal{D}$ by solving the following optimization problem: 
\begin{equation}
q^*(z) = \arg\!\min_{q(\mathbf{z}) \in \mathcal{D}} \kl{(q(\mathbf{z}) || p(\mathbf{z}|\mathbf{x}))}.
\end{equation}

where \textsc{kl} is the KL-divergence, which measures the ``distance'' between two distributions. 

However, (4) requires us to compute the log evidence (which is intractable over the space of all $\mathbf{z}$) since 
\begin{align}
  \kl{q(\bz) \| p(\bz \g \bx)} =
  \E\left[\log q(\bz)\right] -
  \E\left[\log p(\bz, \bx)\right] +
  \log p(\bx). 
\end{align}

Instead, we optimize an objective function which is not dependent on $\log p(\bx)$. We call this the evidence lower bound (\textsc{ELBO}), which is equal to the negative \textsc{KL}-divergence plus the $\log$ evidence.
\begin{align}
  \ELBO(q) =
  \E{[\log p(\bz, \bx)]} -
  \E{[\log q(\bz)]}.
\end{align}
and thus we see that $\log p(\bx)$ is a constant with respect to $q$. The $\textsc{ELBO}$ gets its name from the fact that it is a lower bound for the $\log$ evidence. 

\subsection{The mean-field variational family}
And what family of distributions do we use for $\mathcal{D}$? The standard technique is to use a simple one from physics, the mean-field variational family. In this family, the latent variables $\bz$ are mutually independent so that the joint distribution factorizes:
\begin{align}
  q(\bz) = \prod_{j=1}^{m} q_j(z_j).
\end{align}
where $q_j$ is a bounded variation dependent only on $z_j$. The structure of the model will dictate the optimal form of $q_j$. 

\subsection{Mean-field assumptions break dependency to give us coordinate ascent}
The optimization is solved using a coordinate ascent algorithm, where the independence of the latent variables gives us orthogonality. Let $z_{-j}$ denote the set of latent variables $z_l$ such that $l \neq j$. Consider the complete conditional of $z_j$, which is a function of the other latent variables and the data, $p(z_j \g \bz_{-j}, \bx)$. Since the expectation in the \textsc{ELBO} is with respect to $q(\bz)$, which we have assumed factorizes, then we can dissect out the dependence with respect to $\bz_j$ by using (6) and (7):
\begin{align}
\ELBO(q) &= \int \prod q_i(\bz_i) \left(\log p(\bz, \bx) - \sum_i \log q_i(\bz_i) \right) \,d\bz \nonumber \\
			&\propto \int q_j(z_j) \E_{-j}\left[\log p(\bx, \bz) \right] \, d\bz_j - \int q_j(z_j) \log q_j(\bz_j) \, d\bz_j
\end{align}
Now suppose that we fix $z_{-j}$ and maximize the \textsc{ELBO}. Then the \textsc{ELBO} is maximized when $\log q_j(\bz_j) \propto \E_{-j}\left[\log p(\bx, \bz) \right]$, by the positivity of the $\textsc{KL}$-divergence. Thus the optimal $q^*(\bz_j)$ occurs when
\begin{align}
q^*_j(\bz_j) \propto \exp\left(\E_{-j}\left[\log p(\bx, \bz) \right]\right)
\end{align}

(9) underlies the coordinate-ascent variational inference algorithm. By iterating through each variational factor, fixing the others, and performing coordinate ascent (similar to Gibbs sampling), then we eventually reach a local optimum of the \textsc{ELBO}.

\begin{algorithm}[h]
\KwIn{A model $p(\bx, \bz)$, a data set $\bx$}
\KwOut{A variational density $q(\bz) = \prod_{j=1}^{m} q_{j}(z_j)$}
\textbf{Initialize:} Variational factors $q_{j}(z_j)$ \\
\While{the \textsc{ELBO} has not converged} {
  \For{$j \in \{1, \ldots, m\}$} {
    Set $q_{j}(z_j) \propto \exp\{\E_{-j}[\log p(z_j \g \bz_{-j}, \bx)]\}$\\
  }
  Compute $\ELBO(q) = \EE{\log p(\bz, \bx)} + \EE{\log q(\bz)}$
}
\Return{$q(\bz)$}
\caption{\textsc{CAVI}}
\label{alg:cavi}
\end{algorithm}

\subsection{Exponential family distributions give us a general CAVI formula}

If our posterior is in the exponential family, then the computation of coordinate ascent and ELBO can be generalized. Recall that a distribution is in exponential form if it can parameterized by $$f_X(x\mid\theta) = h(x) \exp \left (\theta^T \cdot T(x) -A(\theta)\right )$$
where $T(x)$ is the sufficient statistic vector, $\theta$ is the natural parameter vector, and $A(\theta)$ is the cumulant. We keep the actual derivations to (Hughes 2015), but the intuition is that because the optimal variational updates are proportional to a $\exp(\E[\log(.)])$ then writing the distribution in exponential form reveals some dependencies that hold for all exponential family members.



\section{Variational Inference on Multi-sample Binomial Model}

\subsection{The model and its ELBO}

We write the ELBO as a function of the data and latent variables:
\begin{equation}
	\begin{split}
		\ELBO\left(q(\bx, \bz | \gamma, \alpha_0, \beta_0)\right) = &E_q[\log p(\mathbf{v} | \gamma)] + E_q[ \log p(\bm{\phi} | \alpha_0, \beta_0)] + \\ &\sum\limits_{n=1}^N \left( E_q[\log p(z_n | \mathbf{v})] + E_q[\log p(x_n | z_n)]\right) \\  &- E_q[\log q(\bz, \mathbf{v}, \bm{\phi})]
	\end{split}
\end{equation}
where $\lambda$ represents the hyperparameter governing the stick-breaking process, and $\alpha_0, \beta_0$ are the hyperparameters governing the base beta distribution. Suppose that the joint distribution for the last term in the $\ELBO$ factors as follows:
\begin{align}
    q(\bz, \mathbf{v}, \bm{\phi}) &=
\underbrace{\prod\limits_{k=1}^K q_{\tau_t}(\bm{\phi}_k)}_{\text{Observation parameters}} \times
 \underbrace{\prod\limits_{k=1}^K q_{\gamma_t}(\mathbf{v}_k)}_{\text{Cluster proportions}} \times
 \underbrace{\prod\limits_{n=1}^{N} q_{r_n}(z_n)}_{\text{Assignment of data to clusters}} \nonumber
\end{align}

Note that the cluster proportions and data assignments are dictated by the Dirichlet Process|we call this the allocation model. On the other hand, the observation parameters vary depending on the structure of the generative model|we call this the observation model. The equations relating to the allocation model are standard, and the theory is left to (Blei 2006, Hughes 2015). Here we derive the form of the  $q(\bm{\phi}_k)$ is a function of the \textbf{vector} $\bm{\phi_k}$, as the observation model is specific to our multi-sample binomial model.

We note that for an individual allelic site in a sample, the data likelihood is binomial. With a beta prior, we know that the resulting posterior for $q(\bm{\phi}_{mk})$ is conjugate to the binomial, and thus $q(\bm{\phi}_{mk}) \sim \mathrm{Beta}(\bm{\phi}_k | \alpha_{mk}, \beta_{mk})$ where $\alpha_{mk}, \beta_{mk}$ are variational parameters. Because reads across samples at a site are assumed to be independent, then we have 
\begin{align*}
q(\bm{\phi}_k)  &= \prod\limits_{m=1}^M q(\bm{\phi}_{mk}) \\
				&= \prod\limits_{m=1}^M \mathrm{Beta}(\bm{\phi}_k | \alpha_{mk}, \beta_{mk})
\end{align*}

\subsection{Multi-sample Binomial coordinate ascent}
\subsubsection{Allocation model}
The coordinate ascent equations for the allocation model are standard (Blei 2006), as they follow from the fact that the stick-breaking process is in the exponential family. The allocation model has variational parameters $\{\eta_{k0}, \eta_{k1}\}_{k=1}^K$ for the cluster proportions and $\{\hat{r}_{nk}\}_{n=1, k=1}^{N, K}$ for the cluster responsibilities. On each iteration, the coordinate update is
\begin{align}
\eta_{k1} &= 1 + \sum_n \hat{r}_{nk} = 1 + N_k \\
\eta_{k0} &= \gamma + \sum_n \sum_{j=k+1}^K \hat{r}_{nj} = N_k^>\\
\hat{r}_{nk} &\propto \exp(S_k) \\
\intertext{for $n = 1, \ldots, N$, $k=1, \ldots, K$, and where}
S_k &= \E_q[\log \bm{v}_k] + \sum_{i=1}^{k-1} \E_q \log(1 - \bm{v}_i) + \E_q[\log p(x_n | \alpha_{nk}, \beta_{nk})]
\intertext{and}
\E_q[\log \bm{v}_i] &= \Psi(\eta_{k0}) - \Psi(\eta_{k0} +\eta_{k1}) \\
\E_q[\log(1 -  \bm{v}_i)] &= \Psi(\eta_{k1}) - \Psi(\eta_{k0} +\eta_{k1})
\end{align}
The digamma functions come from the fact that derivative of the cumulant is the expectation, and the cumulant of a beta has gamma functions (http://math.stackexchange.com/questions/1603172/digamma-function-in-expectation). Since the $\hat{r}_{nk}$ sum to 1 over $n=1, \ldots, N$ then we renormalize at every step as well.
\subsubsection{Observation model}
Following the derivations in (Hughes 2015), we derive the coordinate ascent equations for our observation model, taking advantage of the fact that $q(\bm{\phi}_k)$ is in the exponential family, which we show below.

\begin{claim}
$q(\bm{\phi}_k)$ is in the exponential family.
\begin{proof}
We know that
\begin{align*}
q(\bm{\phi}_k)  = \prod\limits_{m=1}^M q(\bm{\phi}_{mk}) = \prod\limits_{m=1}^M \mathrm{Beta}(\bm{\phi}_k | \alpha_{mk}, \beta_{mk}).
\end{align*}
The beta distribution is in the exponential family, with parameterization 
\begin{align*}
\begin{split}
\mathrm{Beta}(\bm{\phi}_k |  \alpha_{mk}, \beta_{mk}) =  \frac{1}{\phi_{mk}(1 - \phi_{mk})} \exp \bigg(
		\begin{bmatrix}
			\log\phi_{mk} & \log(1 - \phi_{mk})
		\end{bmatrix}
		\begin{bmatrix}
			\alpha_{mk} \\ \beta_{mk}
		\end{bmatrix} \\
+ \log \Gamma(\alpha_{mk} + \beta_{mk}) - \log \Gamma(\alpha_{mk}) - \log\Gamma(\beta_{mk}) \bigg)
\end{split}
\end{align*}
and thus $q(\bm{\phi}_k)$ is in the exponential family with the form
\begin{align*}
q(\bm{\phi}_k) = \left(\prod\limits_{m=1}^M \frac{1}{\phi_{mk}(1 - \phi_{mk})} \right) \exp\bigg(
			\begin{bmatrix}
				\begin{bmatrix}
				 \log\phi_{1k} & \log(1 - \phi_{1k} )
				\end{bmatrix} & \cdots &
				\begin{bmatrix}
				 \log\phi_{Mk} & \log(1 - \phi_{Mk})
				\end{bmatrix}
			\end{bmatrix}
			\begin{bmatrix}
				\begin{bmatrix}
				\alpha_{1k} \\ \beta_{1k} 
				\end{bmatrix} \\
				\vdots \\
				\begin{bmatrix}
				\alpha_{Mk} \\ \beta_{Mk}
				\end{bmatrix}
			\end{bmatrix} \\
			+ \sum\limits_{m=1}^M \log \Gamma(\alpha_{mk} + \beta_{mk}) - \log \Gamma(\alpha_{mk}) - \log\Gamma(\beta_{mk}) \bigg)
\end{align*}
where we have abused notation to show the tuple nature of the sufficient statistics. Thus, for our variational distribution $q(\bm{\phi}_k)$ we have
\begin{align}
\text{Natural parameters} &=  			\begin{bmatrix}
				\begin{bmatrix}
				\alpha_{1k} \\ \beta_{1k} 
				\end{bmatrix} \\
				\vdots \\
				\begin{bmatrix}
				\alpha_{Mk} \\ \beta_{Mk}
				\end{bmatrix}
			\end{bmatrix} \\
\text{Cumulant} &= \sum\limits_{m=1}^M \log \Gamma(\alpha_{mk} + \beta_{mk}) - \log \Gamma(\alpha_{mk}) - \log\Gamma(\beta_{mk}) \qedhere
\end{align}
\end{proof}
\end{claim}

\subsubsection{Exponential factorization of data model}
\vspace{-0.5cm}
\begin{align}
p(\bx_n | \bz_n) &= \prod\limits_{m=1}^M \mathrm{Bin}(v_{mn}, \phi_{mn}; d_{mn}) \nonumber \\
				 &= \prod\limits_{m=1}^M \exp\left(v_{mn} \log\left(\frac{\phi_{mn}}{1 - \phi_{mn}}\right) + d_{mn} \log(1 - \phi_{mn}) \right) \nonumber \\				 		
\begin{split}
				&= \left( \prod\limits_{m=1}^M \binom{d_{mn}}{v_{mn}} \right)
				\exp\bigg( 
\begin{bmatrix}
         \log\left(\frac{\phi_{1n}}{1 - \phi_{1n}}\right) & \cdots & \log\left(\frac{\phi_{Mn}}{1 - \phi_{Mn}}\right)
        \end{bmatrix}\begin{bmatrix}
         v_{1n} \\ \vdots \\ v_{Mn}
        \end{bmatrix}  + \\
\begin{bmatrix}
         \log\left(1 - \phi_{1n}\right) & \cdots & \log\left(1 - \phi_{Mn}\right)
        \end{bmatrix}      
                \begin{bmatrix}
         d_{1n} \\ \vdots \\ d_{Mn}
        \end{bmatrix} 
        \bigg)
        \end{split} \\
&=    \left( \prod\limits_{m=1}^M \binom{d_{mn}}{v_{mn}} \right)
			 \exp\bigg(
			\begin{bmatrix}
				\begin{bmatrix}
				 v_{1n} &  d_{1n} )
				\end{bmatrix} & \cdots &
				\begin{bmatrix}
				 v_{Mn} &  d_{Mn}
				\end{bmatrix}
			\end{bmatrix}
			\begin{bmatrix}
				\begin{bmatrix}
					\log\left(1 - \phi_{1n}\right) \\ \log\left(\frac{\phi_{1n}}{1 - \phi_{1n}}\right)
				\end{bmatrix} \\
				\vdots \\
				\begin{bmatrix}
					\log\left(1 - \phi_{Mn}\right) \\ \log\left(\frac{\phi_{Mn}}{1 - \phi_{Mn}}\right)
				\end{bmatrix}
			\end{bmatrix} \\
\end{align}
so that the sufficient statistics 
\begin{equation}
T(\bx_n) = 
\begin{bmatrix}
				\begin{bmatrix}
				 v_{1n} &  d_{1n}
				\end{bmatrix} & \cdots &
				\begin{bmatrix}
				 v_{Mn} &  d_{Mn}
				\end{bmatrix}
			\end{bmatrix}
\end{equation}

Thus, following (Hughes 2015) we have the following coordinate ascent updates for the observation model: (natural parameter plus sufficient statistic $S_k^{var}$ or $S_k^{ref}$)
\begin{align*}
\alpha_{mk} =  (\alpha_0 - 1) + \sum_{n=1}^N \hat{r}_{nk} \begin{bmatrix} v_{1n} \\ \vdots \\ v_{Mn} \end{bmatrix} \\
\beta_{mk} = (\beta_0 - 1) + \sum_{n=1}^N \hat{r}_{nk} \begin{bmatrix} d_{1n} \\ \vdots \\ d_{Mn} \end{bmatrix}
\end{align*}

\subsection{Sufficient statistics}
Define 
\begin{align}
S_k &= \sum_{n=1}^N \hat{r}_{nk} s(x_n) = \sum_{n=1}^N \hat{r}_{nk} \begin{bmatrix}
				\begin{bmatrix}
				 v_{1n} &  d_{1n}
				\end{bmatrix} & \cdots &
				\begin{bmatrix}
				 v_{Mn} &  d_{Mn}
				\end{bmatrix}
			\end{bmatrix} \\
N_k &= \sum_{n=1}^N \hat{r}_{nk} \\
N_k^> &= \sum_{k+1}^K N_k
\end{align}

\subsubsection{Obs Model Likelihoods}
\begin{align*}
\E_q[\log p(x_n | 
\end{align*}

\section{Implementation}
This has been implemented in Python.

\section{Evaluating results}

The clusterings will be evaluated on simulated and real tumor data. It may also be used in conjunction with other algorithms which use such clusterings as input, to see if this improves their performance.

\end{document}
