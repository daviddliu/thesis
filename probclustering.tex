\documentclass[11pt]{article}
\usepackage[margin=1in, paperwidth=8.5in, paperheight=11in]{geometry}
\usepackage{epstopdf}
\usepackage[parfill]{parskip}
\usepackage[pdftex]{graphicx}
\usepackage{changepage}
\usepackage{titling}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{bm}
\usepackage{extramarks}
\usepackage{amsfonts}
\usepackage{amsthm}
\usepackage{array}
\usepackage{framed}
\usepackage{xy}
\usepackage{multicol}
\usepackage{hyperref}
\usepackage{float}
\usepackage{subfig}
\usepackage[algoruled]{algorithm2e}
\textwidth 7.0in
\usepackage{fancyhdr}
\usepackage{lastpage}
\pagestyle{fancy}
\headheight 26pt
\fancyhf{}
\linespread{1.1}
\headsep20pt
\newcommand{\inlinecode}{\texttt}
\newcommand{\divider}{\line(1,0){250}}
\rfoot{Page \thepage \  of \pageref{LastPage} }

\renewcommand\headrulewidth{0.4pt}
\renewcommand\footrulewidth{0.4pt}

\setlength\parindent{0pt}

%Custom definitions
\newcommand{\problempart}[1]{\textbf{#1.}}
\newcommand{\pof}[1]{\text{P(#1)}}
\newcommand{\subtitle}[1]{%
      \posttitle{%
              \par\end{center}
                  \begin{center}\large#1\end{center}
                          \vskip0.5em}%
                      }

%%Set symbols
\newcommand{\given}{\ensuremath{|} }
\newcommand{\AND}{\ensuremath{\cap} }
\newcommand{\OR}{\ensuremath{\cup} }

\newcommand{\N}{\mathbb{N}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\Q}{\mathbb{Q}}

\newcommand{\powerset}[1]{\ensuremath{ \mathcal P \left({#1}\right)}}
\newcommand{\set}[1]{\ensuremath{\left\{ #1 \right\}}}
\newcommand{\stcomp}[1]{\overline{#1}} 

\newcommand{\dlim}[2][\infty]{\displaystyle \lim_{#2 \rightarrow #1}}
\newcommand{\indefint}{\displaystyle \int}
\newcommand{\be}{\begin{enumerate}}
\newcommand{\ee}{\end{enumerate}}
\newcommand{\dsum}[2]{\displaystyle \sum_{#1}^{#2} }
\newcommand{\defint}[4]{\int^#2_#1 #3\,d#4}

\newcommand{\bx}{\ensuremath{\mathbf{x}}}
\newcommand{\bz}{\ensuremath{\mathbf{z}}}
\newcommand{\kl}[1]{\textsc{kl}\left(#1\right)}
\newcommand{\g}{\,\vert\,}

\newcommand{\EE}[1]{\mathbb{E}\left[#1\right]}
\newcommand{\EEE}[2]{\mathbb{E}_{#1}\left[#2\right]}
\newcommand{\ELBO}{\textsc{elbo}}

%
% Create Problem Sections
%

\newcommand{\enterProblemHeader}[1]{
    \nobreak\extramarks{}{Problem \arabic{#1} continued on next page\ldots}\nobreak{}
    \nobreak\extramarks{Problem \arabic{#1} (continued)}{Problem \arabic{#1} continued on next page\ldots}\nobreak{}
}

\newcommand{\exitProblemHeader}[1]{
    \nobreak\extramarks{Problem \arabic{#1} (continued)}{Problem \arabic{#1} continued on next page\ldots}\nobreak{}
    \stepcounter{#1}
    \nobreak\extramarks{Problem \arabic{#1}}{}\nobreak{}
}

\setcounter{secnumdepth}{0}
\newcounter{partCounter}
\newcounter{homeworkProblemCounter}
\setcounter{homeworkProblemCounter}{1}
\nobreak\extramarks{Problem \arabic{homeworkProblemCounter}}{}\nobreak{}

%
% Homework Problem Environment
%
% This environment takes an optional argument. When given, it will adjust the
% problem counter. This is useful for when the problems given for your
% assignment aren't sequential. See the last 3 problems of this template for an
% example.
%
\newenvironment{homeworkProblem}[1][-1]{
    \ifnum#1>0
        \setcounter{homeworkProblemCounter}{#1}
    \fi
    \section{Problem \arabic{homeworkProblemCounter}}
    \setcounter{partCounter}{1}
    \enterProblemHeader{homeworkProblemCounter}
}{
    \exitProblemHeader{homeworkProblemCounter}
}

\renewcommand{\part}[1]{\textbf{\large Part \Alph{partCounter}}\stepcounter{partCounter}\vspace{4pt}  \\}

% Various Helper Commands
%

\newcommand{\adjustimg}{% Horizontal adjustment of image
  \checkoddpage%
  \ifoddpage\hspace*{\dimexpr\evensidemargin-\oddsidemargin}\else\hspace*{-\dimexpr\evensidemargin-\oddsidemargin}\fi%
}
\newcommand{\centerimg}[2][width=\textwidth]{% Center an image
  \makebox[\textwidth]{\adjustimg\includegraphics[#1]{#2}}%
}


% Useful for algorithms
\newcommand{\alg}[1]{\textsc{\bfseries \footnotesize #1}}

% For derivatives
\newcommand{\deriv}[1]{\frac{\mathrm{d}}{\mathrm{d}x} (#1)}

% For partial derivatives
\newcommand{\pderiv}[2]{\frac{\partial}{\partial #1} (#2)}

% Integral dx
\newcommand{\dx}{\mathrm{d}x}

% Alias for the Solution section header
\newcommand{\solution}{\textbf{\large Solution}}

% Probability commands: Expectation, Variance, Covariance, Bias
\newcommand{\E}{\mathrm{E}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\Bias}{\mathrm{Bias}}

\newcommand{\matr}[1]{\mathbf{#1}}

\makeatletter
\def\thm@space@setup{%
  \thm@preskip=0.4cm
  \thm@postskip=\thm@preskip % or whatever, if you don't want them to be equal
}
\makeatother


\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{proposition}{Proposition}
\newtheorem{observation}{Observation}
\newtheorem{proposal}{Proposal}
\newtheorem{claim}{Claim}

% Move qed box to the left
\renewcommand{\qed}{\unskip\nobreak\quad\qedsymbol}

\newenvironment{conditions}
  {\par\vspace{\abovedisplayskip}\noindent\begin{tabular}{>{$}l<{$} @{${}={}$} l}}
  {\end{tabular}\par\vspace{\belowdisplayskip}}

% Title page

  \title{Clustering SNVs for Tumor Heterogeneity}
  \subtitle{Math-CS Sc.B Thesis Proposal}
  \date{\today}
\author{David Liu}
% Header

\begin{document}
\maketitle
\section{Problem statement}
Cancer results from an evolutionary process where somatic mutations occur and accumulate in a population of cells. A mutation causes genetic variation at a genomic site called a single nucleotide variant (SNV) for the cell which obtained the mutation and all of its progeny. Thus, combinations of SNVs correspond to different subpopulations of cells (clones) which can be placed on a phylogenetic tree. This mixture of clones is a phenomenon known as intratumor hetereogeneity. Each clone is commonly modeled by a binary feature vector, where each feature is an SNV.

So suppose we take biopsy samples from a tumor seperated spatially or temporally. Each sample will have different proportions of clones whose relationships are unknown. Our goal is to characterize the evolutionary history of the clones (invariant across samples) and their mixing proportions (variant across samples). 

The data typically used for tree inference is called the variant allele frequency (VAF), observed as follows. Suppose we bulk sequence each sample seperately, so that the reads are from a sample's mixture of clones. By comparing to a control sample, if a read contains the mutated allele at a SNV, it is called a variant read; otherwise it is called a reference read. The VAF is defined for each SNV in each sample, as, in each sample, the number of variant reads at an SNV divided by the number of total reads at that SNV.

There exist algorithms to infer a tree and its population mixing proportions given perfectly accurate VAFs. However in the real world, data is noisy and this isn't so simple. Two VAFs that look different may actually be from the same clone; likewise, two VAFs that look the same may be from different clones. Thus, it is common to perform clustering to assign mutations to clusters which correspond to clones. Tree inference is then performed on these clusters.

In this thesis, I propose and implement a method to cluster mutations using the Dirichlet Process and variational inference based on a binomial mixture model, which is suited for the clone mixing problem. It could also have applications in other clustering problems.

\newpage

\section{Model}

Let $m = 1, \ldots, M$ index the samples and let $n = 1, ..., N$ index the SNVs. For each SNV $n$ in sample $m$, we observe two quantities: the total number $d_{mn}$ of reads and the number $v_{mn}$ of variant reads. Let each data observation be a vector $\bx_n$, a vector which encapsulates $d_{mn}, v_mn$ for all $m$ and this $n$. Let $k=1,\ldots,K$ index the clusters and let $z_n$ denote the cluster assignment of SNV $n$, where $z_n$ is a 1-of-$K$ indicator vector. In addition, let $\bz$ be the vector of all $z_n$.

Further suppose that the cluster memberships $z_n$ and weights $\pi_k$ are generated by a Dirichlet process prior. Thus $K$ could be countably infinite, but with probability one, $K$ is finite. Thus, we restrict $K$ to a positive number, with all cluster indices greater than $K$ being irrelevant.

Suppose that each clone emits variant reads according to a binomial distribution. Thus, for cluster $k$ and some reads $d_{mn}$, we have variant reads distributed with $\mathrm{Bin}(d_{mn}, \phi_{kz_{nk}})$. (\textbf{Note: May change this into a negative binomial model.}) We can consider the joint probability of a site by the product of each sample, since we assume they are independent. That is,
\begin{equation}
p(\mathbf{x_n} | \mathbf{\phi_n}) = \prod\limits_{m=1}^M \mathrm{Bin}(d_{mn}, \phi_{mn})
\end{equation}

 As we can see by inspecting the graphical model, the likelihood of $\bx$ depends on the latent variables in a straightforward way:
\begin{align}
p(\mathbf{x_n} | \bz, \mathbf{\phi}) &= \prod\limits_{k=1}^K p(\mathbf{x_n} | \mathbf{\phi_n})^{z_{nk}} \nonumber \\
										&= \prod\limits_{k=1}^K \prod\limits_{m=1}^M \mathrm{Bin}(d_{mn}, \phi_{mn})^{z_{nk}}
\end{align}
Now consider the joint likelihood of the observed data and latent variables, which follows from (2): 
\begin{equation}
p(\mathbf{x_n}, \bz | \mathbf{\pi}, \mathbf{\phi_n}) = \prod\limits_{k=1}^K \prod\limits_{m=1}^M \left(\pi_k \mathrm{Bin}(d_{mn}, \phi_{mn})\right)^{z_{nk}}
\end{equation}

Figure~\ref{fig:GM} on the next page shows the graphical model.

\newpage
\begin{quote}
$n = 1, \ldots , N$:  SNVs \\
$m = 1, \ldots , M$: samples \\
$k = 1, \ldots ,  K$: clusters
\end{quote}

\begin{figure}[H]
\centerimg[scale=1.0]{multi_pgm.png}
\caption{Graphical model for the VAFs.}
\label{fig:GM}
\end{figure}

\begin{quote}
$DP$ = Dirichlet process RV (stick-breaking construction) \\
$H \sim U(0,1)$ = Base distribution for parameters $\phi_{nk}$	 \\
$\pi_k$ = Weights for categorical distribution \\
$z_{n} \in \{1, \cdots, K\}  \sim \mathrm{Categorical}(\pi_1, \cdots, \pi_K)$ =  Cluster membership for SNV $n$\\
$\phi_{mk}$ = Cluster frequency \\ 
$v_{mn} \sim \mathrm{Binom}(d_{m,n}, \phi_{m,c_n})$ = Observed variant reads\\ 
$d_{mn}$ = Observed total reads
\end{quote}
\vspace{0.05cm}

As described in (Blei 2006), the data arises in the following manner, with a stick-breaking construction given as:
\begin{align*}
\pi_i(\mathbf{w}) &= w_i \prod\limits_{j=1}^{i-1} (1 - v_j) \\
				DP &= \sum_{i=1}^\infty \pi_i(\mathbf{w}) \delta_{\phi_i}
\end{align*}
\begin{enumerate}
	\item Draw $W_k | \alpha \sim \mathrm{Beta}(1, \alpha)$, \hspace{1cm} $k = \{1, 2, \ldots\}$
	\item Draw $\phi_k | H \sim H^k$, \hspace{1cm} $k = \{1, 2, \ldots\}$
	\item For the $n$th data point:
	\begin{enumerate}
		\item Draw $z_n | \{\pi_1, \pi_2, \ldots\} \sim \mathrm{Mult}(\pi{\mathbf{w}})$.
		\item Draw $\mathbf{w_n} | z_n \sim p(x_n | \phi_n)$.
	\end{enumerate}
\end{enumerate}
\vspace{0.05cm}
The full posterior 
\begin{equation}
p(\bz | \bx, \alpha, H) = \int p(\bx | \mathbf{\phi}) p(\phi | \bx, \alpha, H) \, d\phi
\end{equation}
involves a Dirichlet Process and is thus analytically intractable. We must use some sort of computational technique to perform inference on this posterior.

\section{Variational Inference}

Variational inference is an alternative to MCMC-based inference methods. At a high level, variational inference factors a posterior using the mean-field approximation, which approximates the posterior in a higher-dimensional space using simpler independent functions. Then a simple coordinate ascent can be performed in order to infer the model parameters.

\section{Details on Variational Inference}

\subsection{The \textsc{ELBO}}

Let $\mathbf{z}$ denote the latent variables, and $\mathbf{x}$ denote the data. We seek to approximate the posterior $p(\mathbf{z}|\mathbf{x})$ from a family of distributions $\mathcal{D}$ by solving the following optimization problem: 
\begin{equation}
q^*(z) = \arg\!\min_{q(\mathbf{z}) \in \mathcal{D}} \kl{(q(\mathbf{z}) || p(\mathbf{z}|\mathbf{x}))}.
\end{equation}

where \textsc{kl} is the KL-divergence, which measures the ``distance'' between two distributions. 

However, (4) requires us to compute the log evidence (which is intractable over the space of all $\mathbf{z}$) since 
\begin{align}
  \kl{q(\bz) \| p(\bz \g \bx)} =
  \E\left[\log q(\bz)\right] -
  \E\left[\log p(\bz, \bx)\right] +
  \log p(\bx). 
\end{align}

Instead, we optimize an objective function which is not dependent on $\log p(\bx)$. We call this the evidence lower bound (\textsc{ELBO}), which is equal to the negative \textsc{KL}-divergence plus the $\log$ evidence.
\begin{align}
  \ELBO(q) =
  \E{\log p(\bz, \bx)} -
  \E{\log q(\bz)}.
\end{align}
and thus we see that $\log p(\bx)$ is a constant with respect to $q$. The $\textsc{ELBO}$ gets its name from the fact that it is a lower bound for the $\log$ evidence. 

\subsection{The mean-field variational family}
And what family of distributions do we use for $\mathcal{D}$? The standard technique is to use a simple one from physics, the mean-field variational family. In this family, the latent variables $\bz$ are mutually independent so that the joint distribution factorizes:
\begin{align}
  q(\bz) = \prod_{j=1}^{m} q_j(z_j).
\end{align}
where $q_j$ is a bounded variation dependent only on $z_j$. The structure of the model will dictate the optimal form of $q_j$. 

\subsection{Mean-field assumptions break dependency to give us coordinate ascent}
The optimization is solved using a coordinate ascent algorithm, where the independence of the latent variables gives us orthogonality. Let $z_{-j}$ denote the set of latent variables $z_l$ such that $l \neq j$. Consider the complete conditional of $z_j$, which is a function of the other latent variables and the data, $p(z_j \g \bz_{-j}, \bx)$. Since the expectation in the \textsc{ELBO} is with respect to $q(\bz)$, which we have assumed factorizes, then we can dissect out the dependence with respect to $\bz_j$ by using (6) and (7):
\begin{align}
\ELBO(q) &= \int \prod q_i(\bz_i) \left(\log p(\bz, \bx) - \sum_i \log q_i(\bz_i) \right) \,d\bz \nonumber \\
			&\propto \int q_j(z_j) \E_{-j}\left[\log p(\bx, \bz) \right] \, d\bz_j - \int q_j(z_j) \log q_j(\bz_j) \, d\bz_j
\end{align}
Now suppose that we fix $z_{-j}$ and maximize the \textsc{ELBO}. Then the \textsc{ELBO} is maximized when $\log q_j(\bz_j) \propto \E_{-j}\left[\log p(\bx, \bz) \right]$, by the positivity of the $\textsc{KL}$-divergence. Thus the optimal $q^*(\bz_j)$ occurs when
\begin{align}
q^*_j(\bz_j) \propto \exp\left(\E_{-j}\left[\log p(\bx, \bz) \right]\right)
\end{align}

(9) underlies the coordinate-ascent variational inference algorithm. By iterating through each variational factor, fixing the others, and performing coordinate ascent (similar to Gibbs sampling), then we eventually reach a local optimum of the \textsc{ELBO}.

\begin{algorithm}[h]
\KwIn{A model $p(\bx, \bz)$, a data set $\bx$}
\KwOut{A variational density $q(\bz) = \prod_{j=1}^{m} q_{j}(z_j)$}
\textbf{Initialize:} Variational factors $q_{j}(z_j)$ \\
\While{the \textsc{ELBO} has not converged} {
  \For{$j \in \{1, \ldots, m\}$} {
    Set $q_{j}(z_j) \propto \exp\{\E_{-j}[\log p(z_j \g \bz_{-j}, \bx)]\}$\\
  }
  Compute $\ELBO(q) = \EE{\log p(\bz, \bx)} + \EE{\log q(\bz)}$
}
\Return{$q(\bz)$}
\caption{\textsc{CAVI}}
\label{alg:cavi}
\end{algorithm}

\section{Variational Inference on Multi-sample Binomial Model}

\begin{lemma}
The product of two exponential family functions is also exponential.
\begin{proof}
Exponential family functions can be parameterized by $$f_X(x\mid\theta) = h(x) \exp \left (\theta^T \cdot T(x) -A(\theta)\right ).$$ So suppose we have $f_1, f_2$ which are correspondingly parameterized. Note that $T(x)$ is the same because the sufficient statistic is invariant across a distribution. Then the product is
\begin{align*}
f_1 \times f_2 &= h_1(x) \exp \left (\theta_1^T \cdot T(x) -A(\theta_1)\right ) \times h_2(x) \exp \left (\theta_2^T \cdot T(x) -A(\theta_2)\right )\\
			   &= \tilde{h}(x) \exp \left ((\theta_1 + \theta_2)^T \cdot T(x) -\tilde{A}(\theta_1, \theta+2)\right )
\end{align*}
which is also an exponential family distribution.
\end{proof}
\end{lemma}
\begin{claim}
The (likelihood of $\bx | \bz?$) is also of the exponential family.
\begin{proof}
    Each binomial likelihood is exponential. Then the claim is trivial by lemma 1.
\end{proof}
\end{claim}

\subsection{Exponential factorizations}
\subsubsection{Exponential factorization of data model}
\vspace{-0.5cm}
\begin{align}
p(\bx_n | \bz_n) &= \prod\limits_{m=1}^M \mathrm{Bin}(v_{mn}, \phi_{mn}; d_{mn}) \nonumber \\
				 &= \prod\limits_{m=1}^M \exp\left(v_{mn} \log\left(\frac{\phi_{mn}}{1 - \phi_{mn}}\right) + d_{mn} \log(1 - \phi_{mn}) \right) \nonumber \\
				 \begin{split}
				&= \left( \prod\limits_{m=1}^M \binom{d_{mn}}{v_{mn}} \right)
				\exp\left( 
\begin{bmatrix}
         \log\left(\frac{\phi_{1n}}{1 - \phi_{1n}}\right) & \cdots & \log\left(\frac{\phi_{Mn}}{1 - \phi_{Mn}}\right)
        \end{bmatrix}\begin{bmatrix}
         v_{1n} \\ \vdots \\ v_{Mn}
        \end{bmatrix}  +
\begin{bmatrix}
         \log\left(1 - \phi_{1n}\right) & \cdots & \log\left(1 - \phi_{Mn}\right)
        \end{bmatrix}      
                \begin{bmatrix}
         d_{1n} \\ \vdots \\ d_{Mn}
        \end{bmatrix} 
        \right)
        \end{split}
\end{align}
so that the natural parameter is a vector
\begin{align}
\bm{\eta} = \begin{bmatrix}
         \log\left(\frac{\phi_{1n}}{1 - \phi_{1n}}\right) & \cdots & \log\left(\frac{\phi_{Mn}}{1 - \phi_{Mn}}\right)
        \end{bmatrix}
\end{align}
and sufficient statistics 
\[
T(\bx_n) = 
 \begin{bmatrix}
         v_{1n} \\ \vdots \\ v_{Mn}
        \end{bmatrix}
\]
\subsection{Prior exponential factorization}
\vspace{-0.5cm}
\begin{align}
p(\bm{\phi}) &= \prod\limits_{k=1}^K p(\bm{\phi_k}) \nonumber \\
			&= \prod\limits_{k=1}^K \prod\limits_{m=1}^M \mathrm{Beta}(a_0, b_0) \tag{$a_0 = b_0 = \frac{1}{2}$} \\
			&= \prod\limits_{k=1}^K \prod\limits_{m=1}^M \frac{1}{\phi_{mk}(1 - \phi_{mk})} \exp\left(\alpha_0 \log\phi_{mk} + \beta_0 \log(1 - \phi_{mk}) + \log \Gamma(\alpha_0 + \beta_0) - \log \Gamma(\alpha_0) - \log\Gamma(\beta_0)\right) \nonumber \\
			&= \left(\prod\limits_{k=1}^K \prod\limits_{m=1}^M \frac{1}{\phi_{mk}(1 - \phi_{mk})} \right) \exp\left(
			\begin{bmatrix}
				[\alpha_0 & \beta_0] \\
				\vdots &\vdots \\
				[\alpha_0 & \beta_0]
			\end{bmatrix}
			\begin{bmatrix}
				\begin{bmatrix}
				 \log\phi_{1k} \\ \log(1 - \phi_{1k} 
				\end{bmatrix} & \cdots &
				\begin{bmatrix}
				 \log\phi_{Mk} \\ \log(1 - \phi_{Mk} 
				\end{bmatrix}
			\end{bmatrix}
			+ \text{the product of the beta logs}
			\right)
\end{align}
and here $a_0 = b_0 = \frac{1}{2}$ since we want the prior to be uniform over $[0,1]$.

\subsection{Coordinate ascent equations}

 (Note that $\lambda$ is a hyperparameter such that $\lambda_1, \lambda_2$ chacterize the base distribution.)

We write the ELBO as:
\begin{equation}
	\begin{split}
		\ELBO\left(q(\bx, \bz | \alpha, \lambda)\right) = &E_q[\log p(\mathbf{v} | \alpha)] + E_q[ \log p(\mathbf{\phi} | \lambda)] + \\ &\sum\limits_{n=1}^N \left( E_q[\log p(z_n | \mathbf{v})] + E_q[\log p(x_n | z_n)]\right) \\  &- E_q[\log q(\bz, \mathbf{v}, \mathbf{\phi})]
	\end{split}
\end{equation}
 
 Suppose that the joint distribution for the last term in the $\ELBO$ factors as follows:
\begin{align}
    q(\bz, \mathbf{v}, \mathbf{\phi}) &=
\underbrace{\prod\limits_{k=1}^K q_{\tau_t}(\mathbf{\phi_k})}_{\text{Observation parameters}} \times
 \underbrace{\prod\limits_{k=1}^K q_{\gamma_t}(\mathbf{v_k})}_{\text{Cluster proportions}} \times
 \underbrace{\prod\limits_{n=1}^{N} q_{r_n}(z_n)}_{\text{Assignment of data to clusters}} \nonumber
\end{align}
 where we have exponential family (uniform 0, 1), betas, and multinomials, respectively. 
 
 digamma comes from derivative of the MGF http://math.stackexchange.com/questions/1603172/digamma-function-in-expectation.
 
 Note that $q(\mathbf{\phi_k})$ is a function of the \textbf{vector} $\phi_k$, as it is the product of binomial distributions. 
 
 From here, we follow some derivations in (Blei 2006, p.129) to derive the coordinate ascent equations. The variational factor updates are actually pretty simple. For $k \in \{1, \ldots, K\}$ and $n \in \{1, \ldots, N\}$:
\begin{align}
\gamma_{k, 1} = 1 + \sum_n r_{n, k} \tag{These are the stick-breaking betas} \\
\gamma_{k, 2} = 	\alpha + \sum_n \sum_{j= k + 1}^K r_{n, j} \tag{This is also for the stick-breaking betas} \\
\tau_{k, 1} = \lambda_1 + \sum_n r_{n, k} x_n \tag{Look at exp defn on p125, this is for the cluster parameter from the base dist} \\
\tau_{k, 2} = \lambda_2 + \sum_n r_{n, k} \tag{Also for the cluster parameter}\\
r_{n, t} \propto \exp(S_t) \tag{For the cluster assignments}
\end{align}
where the details of calculating $S_k$ are on page 129 of (Blei 2006). 

\section{Implementation}
This will either be implemented in Python or R.

\section{Evaluating results}

The clusterings will be evaluated on simulated and real tumor data. It may also be used in conjunction with other algorithms which use such clusterings as input, to see if this improves their performance.

\end{document}
